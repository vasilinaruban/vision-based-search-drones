# Бортовая система компьютерного зрения для БВС 

Репозиторий содержит код бортовой системы компьютерного зрения для беспилотного воздушного судна (БВС)

Система решает задачу поиска людей в природной среде: обработка видеопотока и данных навигации выполняется **полностью на борту** БВС, а на карту памяти записываются только итоговые изображения и файл с координатами найденных объектов.

---

## Содержание

- [Описание проекта](#описание-проекта)
- [Структура репозитория](#структура-репозитория)
- [Setup](#setup)
- [Данные](#данные)
- [Train](#train)
- [Production preparation](#production-preparation)
- [Infer](#infer)
- [Подготовка данных для Судей](#подготовка-данных-для-судей)
- [Статус реализации](#статус-реализации)

---

## Описание проекта

### Задача

Создать бортовую систему, которая в ходе полёта БВС:

1. Захватывает видеопоток с целевой камеры и данные навигации (GPS/ГНСС, курс, высота).
2. Выполняет инференс нейросетевой модели детекции людей на бортовом вычислителе (RK3588, NPU).
3. По результатам детекции оценивает координаты объектов на местности.
4. Формирует изображения только с найденными объектами и файл `results.json` со списком объектов и их координатами.
5. Записывает результаты на карту памяти, откуда они передаются Судьям после завершения попытки.

### Соответствие техническому регламенту

Согласно техрегламенту КОЗ № 2, вся обработка должна происходить **исключительно на борту БВС**, а команда передаёт Судьям только Изображения и файл с данными об объектах поиска

В проекте это обеспечивается следующим образом:

- Нейросеть выполняется на бортовом вычислителе RK3588 через библиотеку RKNN (модуль `orange.RK3588`, `src.models.models.Net`).  
- Постобработка переводит координаты рамок в географические координаты с учётом высоты, FOV и курса БВС, а затем кластеризует объекты и формирует JSON со списком объектов.  
- Файл `pack_results.py` пакует JPEG‑изображения и JSON в `results.zip` и считает SHA256‑хэш в `checksum.txt`, чтобы корневая папка соответствовала требованиям техрегламента (наличие `results.zip`, `checksum.txt` и GPX‑трек полёта).  

---

## Структура репозитория

Основные директории и файлы:

```text
.
├── configs
│   ├── main.yaml          # Общая конфигурация эксперимента (устройство, логгер, пути)
│   ├── data.yaml          # Описание датасета (формат YOLO, классы и пути)
│   ├── model.yaml         # Конфиг модели YOLOv8
│   ├── training.yaml      # Гиперпараметры обучения
│   └── inference.yaml     # Настройки онлайнового инференса (источник видео, модель и т.п.)
│
├── src
│   ├── __init__.py        # Пакет с кодом проекта
│   ├── cli.py             # Точки входа `vision-train` и `vision-infer` (Hydra + configs):contentReference[oaicite:5]{index=5}
│   ├── training.py        # Обёртка над Ultralytics YOLO + DVC + MLflow:contentReference[oaicite:6]{index=6}
│   └── main.py            # Онлайновый инференс на RK3588 (камера → NPU → постпроцессинг):contentReference[oaicite:7]{index=7}
│
├── base
│   ├── __init__.py        # Экспорт Camera / RK3588
│   ├── camera.py          # Захват видео и данных GPS/курса, отправка в очередь
│   ├── rtsp_and_gps.py    # Вспомогательный скрипт захвата RTSP и GPS на диск
│   └── orange.py          # Обёртка над rknnlite: RK3588, управление процессами инференса:contentReference[oaicite:8]{index=8}
│
├── utils
│   ├── __init__.py        # Экспорт `PostProcess` и `Visualizer`
│   └── post_process.py    # Постобработка, вычисление координат, кластеризация, JSON + JPEG:contentReference[oaicite:9]{index=9}
│
├── notebooks
│   ├── train_yolov8_bpla.ipynb   # Исследование обучения YOLOv8
│   └── ONNXtoRKNN.ipynb          # Экспорт PyTorch → ONNX → RKNN
│
├── pack_results.py        # Упаковка best‑результатов в results.zip + checksum.txt:contentReference[oaicite:10]{index=10}
├── pyproject.toml         # Описание зависимостей и CLI‑скриптов (`vision-train`, `vision-infer`):contentReference[oaicite:11]{index=11}
├── .pre-commit-config.yaml# Настройки линтеров/форматтеров
└── README.md
```

В процессе работы создаются дополнительные директории (игнорируются Git):

* `plots/` — сохранённые графики обучения.
* `results_json/` — изображения с разметкой и файлы `object_data_*.json` для найденных объектов.
* `results_for_zip/` — временная папка для подготовки архива `results.zip`.

---

## Setup

### Требования

* Python 3.8+
* ОС Linux (целевое устройство — одноплатный компьютер на RK3588).
* Для обучения:

  * PyTorch / torchvision
  * Ultralytics YOLOv8
  * Hydra + OmegaConf
  * DVC (для загрузки датасета)
  * MLflow (логирование экспериментов)
* Для онлайнового инференса на борту:

  * `rknn-toolkit-lite` / `rknnlite` (устанавливается по инструкции производителя, не через PyPI).

Все основные зависимости для обучения и оффлайн‑разработки описаны в `pyproject.toml`.

### Создание окружения и установка зависимостей

**Вариант через pip (универсальный):**

```bash
python -m venv .venv
source .venv/bin/activate      # Windows: .venv\Scripts\activate

pip install -e ".[dev]"
```

`"[dev]"` подтянет дополнительные зависимости для разработки (pytest, jupyter и т.п.).

**Рекомендуемый вариант через uv (для ДЗ MLsysd):**

```bash
# Установка uv — см. документацию uv
uv sync               # создаёт окружение и ставит зависимости из pyproject.toml
uv run pre-commit install
```

### Настройка pre‑commit

```bash
pre-commit install
pre-commit run -a
```

Все коммиты будут автоматически проверяться форматтерами и линтерами, указанными в `.pre-commit-config.yaml`.

---

## Данные

Описание датасета находится в `configs/data.yaml`. Там задаются:

* корневая директория (`dataset.path`), по умолчанию она может зависеть от переменной окружения;
* относительные пути к подкаталогам `train`, `val`, `test` в формате YOLO;
* количество классов и список имён классов (например, один класс `person`).

Датасет хранится под управлением DVC:

* В репозитории присутствуют DVC‑метаданные, а сами изображения не хранятся в Git.
* При первом запуске обучения вызывается `dvc pull` из корня проекта, если директория с датасетом не существует.

Вы можете также явно выполнить:

```bash
dvc pull
```

после настройки удалённого DVC‑хранилища.

---

## Train

### Конфигурация обучения

Гиперпараметры задаются через Hydra‑конфиги:

* `configs/main.yaml` — базовые настройки эксперимента:

  * устройство (`experiment.device`: `"cuda"` / `"cpu"`),
  * настройки логгера MLflow (`experiment.logger.tracking_uri`, `experiment.logger.experiment_name`),
  * пути для артефактов.
* `configs/model.yaml` — тип модели YOLOv8, флаг `pretrained` и др.
* `configs/training.yaml` — количество эпох, размер батча, размер входного изображения и т.д.
* `configs/data.yaml` — структура датасета.

### Запуск обучения

После активации виртуального окружения:

```bash
uv runvision-train
```

Эта команда:

1. Загружает объединённый конфиг через Hydra (`configs/main.yaml` и связанные файлы).
2. Проверяет наличие директории с датасетом. Если её нет — пытается выполнить `dvc pull`.
3. Генерирует вспомогательный YOLO‑конфиг `configs/yolo_data_autogenerated.yaml` с путями до train/val/test и именами классов.
4. Настраивает MLflow (tracking URI и название эксперимента) и логирует гиперпараметры, включая git‑коммит.
5. Запускает обучение модели YOLOv8 с помощью библиотеки Ultralytics.
6. Собирает графики обучения (метрики, лоссы) и сохраняет их в директорию `plots/`, а также логирует в MLflow.

### Где смотреть результаты

* **Весы модели** и стандартные артефакты Ultralytics YOLO — в каталоге `runs/detect/...` внутри проекта (точный путь возвращается объектом `results.save_dir`).
* **Графики обучения** — в директории `plots/`.
* **Подробные метрики и артефакты эксперимента** — в MLflow UI по адресу, указанному в `configs/main.yaml` (по умолчанию `http://127.0.0.1:8080`). 

---

## Production preparation

В этом разделе описаны шаги подготовки обученной модели к работе на целевом устройстве (RK3588).

### 1. Экспорт PyTorch → ONNX

После обучения у вас есть файл весов YOLOv8, например:

```text
runs/detect/exp/weights/best.pt
```

Экспортировать модель в ONNX можно через Ultralytics:

```bash
python -c "from ultralytics import YOLO; YOLO('runs/detect/exp/weights/best.pt').export(format='onnx', imgsz=640)"
```

На выходе получится файл `best.onnx`.

### 2. Конвертация ONNX → RKNN

Для целевой платформы используется RKNN‑формат:

1. Откройте ноутбук `ONNXtoRKNN.ipynb`.
2. Укажите путь к `best.onnx`.
3. Выполните ячейки конвертации в `.rknn` с нужными параметрами квантования.
4. Полученный файл, например `yolov8s.rknn`, положите в директорию с моделями на хосте/одноплатнике (по умолчанию `checkpoints/`).

### 3. Регистрация модели в коде

В файле `src.models.models` есть словарь `RKNN_MODEL_NAMES` с путями до доступных RKNN‑моделей и их входными размерами.

Добавьте туда свою модель, например:

```python
RKNN_MODEL_NAMES = {
    "YOLO": {
        "path": DEFAULT_MODELS_PATH / "yolov8s.rknn",
        "input_size": 640,
    },
}
```

### 4. Поставка для целевого устройства

На целевое устройство должны быть установлены:

* этот Python‑пакет (установка `pip install -e .` из репозитория),
* библиотеки и драйверы RKNN (`rknn-toolkit-lite` / `rknnlite` и зависимые пакеты),
* файл модели `.rknn` в директории `checkpoints/` или пути, указанном в `RKNN_MODEL_NAMES`.

---

## Infer

### Онлайновый инференс на борту

Основной сценарий исполнения на целевом устройстве реализован в функции `run_online_inference` (`src/main.py`) и вызывается через CLI‑команду:

```bash
vision-infer
```

По умолчанию:

* источник видеопотока `source=0` (локальная камера),
* используется модель `"YOLO"`.

Эти параметры можно изменить в `configs/inference.yaml` (например, задать RTSP‑строку или имя другой модели).

#### Что происходит при запуске

1. Загружается конфигурация через Hydra (так же, как в Train).
2. Создаётся очередь `Queue(maxsize=3)` для обмена кадрами между процессами.
3. Объект `Camera` начинает:

   * захватывать кадры с камеры/RTSP,
   * получать GPS‑координаты и данные курса от автопилота через MAVLink,
   * складывать `(frame, inference_input, gps_data)` в очередь.
4. Объект `RK3588` грузит RKNN‑модель через `rknnlite` и запускает несколько процессов инференса, читающих из очереди и пишущих в выходную очередь.
5. `PostProcess`:

   * декодирует выходы модели (NMS), получает боксы и уверенности,
   * преобразует боксы в геокоординаты с учётом высоты, FOV и курса БВС (`pix2m`, `xyxy2coords`),
   * накапливает результаты по кадрам, выполняет кластеризацию (DBSCAN) и группирует обнаружения по объектам,
   * периодически сохраняет промежуточные JSON‑файлы и лучшие кадры в `results_json/`.
6. `Visualizer` рисует рамки и подписи на кадре и отображает поток для отладки.

### Формат выходных данных

В процессе полёта формируются:

* директория `results_json/`;
* изображения с найденными объектами `*.jpg`;
* файлы `object_data_0.json`, `object_data_1.json` (поочерёдно, чтобы не переполнить память).

Структура JSON (упрощённый пример):

```json
{
  "Objects": [
    {
      "name": "object_0",
      "dd.dddddd_lat": "55.000000",
      "dd.dddddd_lon": "82.000000",
      "images": ["img_00123.jpg", "img_00456.jpg"]
    }
  ]
}
```

Эта структура соответствует требованиям техрегламента: список объектов, для каждого — имя, координаты в формате `dd.dddddd` и перечень JPEG‑изображений с этим объектом.

---

## Подготовка данных для Судей

После завершения попытки:

1. Убедитесь, что в директории `results_json/` есть файлы `object_data_*.json` и связанные изображения `*.jpg`.

2. На борту или после копирования данных на ноутбук выполните:

   ```bash
   python pack_results.py
   ```

   Скрипт:

   * выберет самый свежий `object_data_*.json`;
   * создаст `results_for_zip/`, запишет в него `results.json` и скопирует туда все `*.jpg`;
   * упакует содержимое `results_for_zip/` в `results.zip` в корне проекта;
   * посчитает SHA256‑хэш архива и запишет его в `checksum.txt`.

3. Скопируйте **GPX‑файл трека полёта** автопилота в ту же корневую директорию, где лежат:

   ```text
   checksum.txt
   results.zip
   *.gpx
   ```

   Именно такой состав корневой папки ожидают Судьи согласно техрегламенту; отсутствие данных, их нечитаемость или неверная структура JSON аннулируют результат попытки.

4. Передайте карту памяти Судье в соответствии с процедурой, описанной организаторами.

---

## Статус реализации

Готово:

* Полноценный пайплайн обучения YOLOv8 с конфигурированием через Hydra, хранением данных через DVC и логированием экспериментов в MLflow.
* Онлайновый инференс на борту (камера + GPS → RKNN → постобработка → изображения + JSON).
* Скрипт упаковки результатов в формат, соответствующий техрегламенту (results.zip + checksum.txt + *.gpx).

Планируется:

* Дополнительная отладка и тестирование на реальной площадке Испытаний.
* Тонкая настройка порогов детекции и кластеризации под реальные условия.
* Расширение набора метрик и мониторинг качества в полётах.

---

## Ссылки на ключевые библиотеки (для удобства)

```text
PyTorch:             https://pytorch.org
Ultralytics YOLOv8:  https://docs.ultralytics.com
Hydra:               https://hydra.cc
DVC:                 https://dvc.org
MLflow:              https://mlflow.org
```
---

Использованные файлы и требования: `pyproject.toml` (зависимости и CLI), `configs/*.yaml` (Hydra‑конфиги), `src/training.py` (обучение + DVC + MLflow), `src/main.py` и `utils/post_process.py` (онлайновый инференс и структура JSON), `pack_results.py` и технический регламент КОЗ № 2 (формат результатов и структура корневой папки).
